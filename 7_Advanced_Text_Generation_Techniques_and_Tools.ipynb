{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4.**"
      ],
      "metadata": {
        "id": "pCNGH2TNF2ag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2--M0Gn3FhY2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading an LLM"
      ],
      "metadata": {
        "id": "azahTKR-Gqqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
        "\n",
        "# If this command does not work for you, you can use the link directly to download the model\n",
        "# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6aDGui2Giaz",
        "outputId": "70e495b6-4da6-4c70-f6d4-8fb1b065c6fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-20 19:11:05--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.55, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1737403866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzQwMzg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=WUfy-lJPfwmiozNYmZSsempKzhP2tybInXHiEmCYubv3ufdSFADpGWl2SfJNoMJ-uEiupOjLqle1BVPKS%7EaDO-dbu9KQ8fmgsXrL47vKub8Z8%7EgVh0NMjZNhsLNWqx1ulhqWtO7F-mfiuEcY-OO7aDSfDoLpzIex8wx5MB8VzjYRYWjD2hEUaozMHUGxU6k8NSERst16fJ88ZZ4PdUuH0umA-ilLRFMbkuNj37Y3sphs5CEiRWvfthpqGE2xVyD1LiyeOQdVzigLJwmXEV9bxSVyRn4tRdir6R3NqESMKbQH6W-ijlC0tOEuGTpeauMfxnnDbiWxJF-Pgzb5AMKdVA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-01-20 19:11:06--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1737403866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzQwMzg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=WUfy-lJPfwmiozNYmZSsempKzhP2tybInXHiEmCYubv3ufdSFADpGWl2SfJNoMJ-uEiupOjLqle1BVPKS%7EaDO-dbu9KQ8fmgsXrL47vKub8Z8%7EgVh0NMjZNhsLNWqx1ulhqWtO7F-mfiuEcY-OO7aDSfDoLpzIex8wx5MB8VzjYRYWjD2hEUaozMHUGxU6k8NSERst16fJ88ZZ4PdUuH0umA-ilLRFMbkuNj37Y3sphs5CEiRWvfthpqGE2xVyD1LiyeOQdVzigLJwmXEV9bxSVyRn4tRdir6R3NqESMKbQH6W-ijlC0tOEuGTpeauMfxnnDbiWxJF-Pgzb5AMKdVA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.97, 18.164.174.98, 18.164.174.19, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G  40.1MB/s    in 3m 2s   \n",
            "\n",
            "2025-01-20 19:14:08 (40.0 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "VJTBgJeuGxZn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Hi! My name is Harsh. What is 1 + 1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_pvuPYE9GzFj",
        "outputId": "b8b7e835-14ab-45bb-fea3-a6cdc08b9be3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "-p9oVz2nK6VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt template with the \"input_prompt\" variable\n",
        "template = \"\"\"<s><|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ],
      "metadata": {
        "id": "myiPfzobJxQs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain = prompt | llm"
      ],
      "metadata": {
        "id": "9UvRzHU-LBvH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the chain\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Harsh. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f_PUi0pQLDuh",
        "outputId": "a82c3cc4-c0e1-4e92-a077-2aa261e58d31"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Harsh! The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units in total.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Chains"
      ],
      "metadata": {
        "id": "7nDhhC0dLLDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create a chain for the title of our story\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoRtk_fiLHUd",
        "outputId": "8e5743b2-e37a-4e76-f7db-959de8d6059a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-61dd782c6da9>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title.invoke({\"summary\": \"a boy that lost his father\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7pFG_SHLQ52",
        "outputId": "ab9e32a9-6094-49d1-e1f7-1c4f824c4b19"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a boy that lost his father',\n",
              " 'title': ' \"The Invisible Thread: A Father\\'s Loss, a Boy\\'s Journey\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ],
      "metadata": {
        "id": "KRJak8G1LWlK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the story using the summary, title, and character description\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ],
      "metadata": {
        "id": "mwYURo0eLcY-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all three components to create the full chain\n",
        "llm_chain = title | character | story"
      ],
      "metadata": {
        "id": "3qYhbC07Ld1N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke(\"a boy that lost his father\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thuQkvUJLfor",
        "outputId": "54860d0a-9fcd-4813-d9ba-82d4adbd4af4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a boy that lost his father',\n",
              " 'title': ' \"Finding Light in the Shadows: A Father\\'s Legacy\"',\n",
              " 'character': ' The protagonist, Ethan, is an introspective and resilient young boy who struggles to navigate life without his loving yet absent father. Throughout the narrative, he discovers strength within himself as he uncovers hidden messages left by his late father that guide him on a transformative journey of self-discovery and acceptance.',\n",
              " 'story': \" Finding Light in the Shadows: A Father's Legacy tells the heartfelt tale of Ethan, an introspective young boy grappling with the void left by his father's untimely passing. Bereft of his loving yet absent presence, he navigates a tumultuous world filled with shadows and uncertainty. However, as Ethan sifts through forgotten memories and overlooked treasures in his late father's study, he stumbles upon cryptic notes that reveal the depth of his father’s wisdom, love, and guidance. Each message serves as a beacon illuminating Ethan's path towards self-discovery and resilience, transforming him into an embodiment of strength and acceptance in honor of his late father's legacy. Through this journey, he learns the true meaning of familial bonds and the enduring light that a loving spirit can leave behind.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory"
      ],
      "metadata": {
        "id": "ZRwAdnADLvjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's give the LLM our name\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Harsh. What is 1 + 1?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4cfKvv5aLrAF",
        "outputId": "89403f54-e85e-496f-d87a-6ae270cff430"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The answer to 1 + 1 is 2. It's a basic arithmetic addition where if you have one item and add another item of the same kind, you end up with two items in total.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we ask the LLM to reproduce the name\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "XoO-23faL6eW",
        "outputId": "145dade9-9f98-424c-b11b-8896ae1b5fc6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm unable to determine your name as I don't have the ability to access personal data. However, if you provide more context or information that doesn't involve privacy concerns, I can certainly help with a wide range of topics!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBuffer"
      ],
      "metadata": {
        "id": "GHtNudw3MAnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an updated prompt template to include a chat history\n",
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ],
      "metadata": {
        "id": "6Q_yUzkPL8_p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define the type of Memory we will use\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "t3ZtM2RKMDJd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a conversation and ask a basic question\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Harsh. What is 1 + 1?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWE91UE1ME2G",
        "outputId": "3e0835a6-e10f-4e1a-eeb4-bb71c227f2bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Harsh. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': ' Hello Harsh! The sum of 1 + 1 is 2. How can I assist you further today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Does the LLM remember the name we gave it?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNy6qcLkMHoD",
        "outputId": "91607710-ca60-4807-eaf8-3e642d5feee3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': 'Human: Hi! My name is Harsh. What is 1 + 1?\\nAI:  Hello Harsh! The sum of 1 + 1 is 2. How can I assist you further today?',\n",
              " 'text': ' Your name is mentioned as Harsh in the current conversation.\\n\\nAnd if asked \"What is my name?\" the answer would be: You have introduced yourself as Harsh.'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBufferMemoryWindow"
      ],
      "metadata": {
        "id": "JmJABM_SMfAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Retain only the last 2 conversations in memory\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWHGQ1YOMKTZ",
        "outputId": "fe760c83-65bc-42a7-fc4e-c5f0da1dcc78"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-046ef635f261>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask two questions and generate two conversations in its memory\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Harsh and I am 29 years old. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfsvgpMtMjFY",
        "outputId": "ec0f6115-5af9-415e-9698-2876438ee78e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Harsh and I am 29 years old. What is 1 + 1?\\nAI:  Hello, Harsh! It's nice to meet you. To answer your question directly, 1 + 1 equals 2.\\n\\nHowever, this also presents an opportunity for a brief conversation:\\n\\nIf you have any other questions or topics you'd like to discuss, feel free to let me know!\",\n",
              " 'text': \" Hello Harsh! It's great to meet you as well. To answer your new question directly, 3 + 3 equals 6. And if there are more questions or topics you're interested in discussing, I'm here to help!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether it knows the name we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmOiWxEZMnsE",
        "outputId": "6e99a09e-4468-4035-df13-08d9bbe22eb0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Harsh and I am 29 years old. What is 1 + 1?\\nAI:  Hello, Harsh! It's nice to meet you. To answer your question directly, 1 + 1 equals 2.\\n\\nHowever, this also presents an opportunity for a brief conversation:\\n\\nIf you have any other questions or topics you'd like to discuss, feel free to let me know!\\nHuman: What is 3 + 3?\\nAI:  Hello Harsh! It's great to meet you as well. To answer your new question directly, 3 + 3 equals 6. And if there are more questions or topics you're interested in discussing, I'm here to help!\",\n",
              " 'text': ' Hi Harsh! Your name, as mentioned earlier, is Harsh. It was nice meeting you too. If you have any other questions or need assistance with anything else, feel free to ask!'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether it knows the age we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skMNRh_fMzF9",
        "outputId": "e3849ba5-0db6-4aed-db84-4b55db11b749"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is my age?\\nAI:  As an AI, I don't have access to personal data such as your age unless you choose to share it. Your privacy is important, so I can't determine your age without that information. However, if you're looking for general knowledge about ages or age-related questions, feel free to ask!\\nHuman: What is Harsh's age?\\nAI:  I'm unable to provide personal details such as Harsh's age due to privacy considerations. If this refers to a fictional character named Harsh within your conversation, the age isn't specified here and would depend on the context of that story or discussion. For any real person named Harsh, sharing their age would require their consent.\",\n",
              " 'text': \" I'm unable to determine your age as I don't have access to personal data. If you need assistance with a general inquiry about ages, feel free to ask!\\n[No mathematical solution required]\"}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationSummary"
      ],
      "metadata": {
        "id": "2CCToWj1NECS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summary prompt template\n",
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "NrDB-L2HM_L5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# Chain the LLM, prompt, and memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meGcMm5SNGaY",
        "outputId": "d9cb39ac-1948-4ecd-c00d-c25b95b75a57"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-905e44fca19a>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a conversation and ask for the name\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Harsh. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "273mTSusNHu3",
        "outputId": "03b22399-e2b2-4c7e-a354-56819a17d369"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': ' Harsh introduces himself and asks the AI to solve the basic arithmetic problem 1 + 1. The AI responds by stating that the sum is 2, providing examples for better understanding.',\n",
              " 'text': \" I don't have access to personal data unless it's shared with me in our conversation. But you can call me your AI assistant! Regarding the arithmetic problem you mentioned: Yes, when we add 1 and 1 together, the result is indeed 2. For instance, if you had one apple and someone gave you another apple, you would then have two apples.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether it has summarized everything thus far\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdfDlvOdNLJW",
        "outputId": "49bf1ea5-ef17-47d2-d7a9-2191f410fe55"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': \" The conversation begins with Harsh introducing himself to the AI and requesting it solve a basic arithmetic problem - calculating 1 + 1. The AI confirms that the sum is 2, using an example of having one apple and receiving another to illustrate this concept. Later in the discussion, the Human inquires about their name; the AI clarifies its limitation regarding personal data but reiterates it can be referred to as a 'AI assistant'. It also acknowledges Harsh's earlier arithmetic question, affirming that 1 + 1 equals 2.\",\n",
              " 'text': ' The first question you asked was calculating the sum of 1 + 1.'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what the summary is thus far\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l61sHxrsNWtz",
        "outputId": "ae77c8bf-edce-4f7e-db94-d23d3ca15082"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': ' The conversation starts with Harsh introducing himself and asking the AI to solve a basic arithmetic problem, which is finding the sum of 1 + 1. The AI confirms that it equals 2 by providing an example of having one apple and receiving another. Later on, the Human asks about their first question, and the AI reiterates that it was calculating the sum of 1 + 1, which results in 2.'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IDSFi18NbaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "azbvHvZlNe3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load OpenAI's LLMs with LangChain\n",
        "# Read the key from a secure file on your Drive\n",
        "with open('/content/MY_KEY.txt', 'r') as file:\n",
        "    api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "_Vo93qshNfjJ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the ReAct template\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ],
      "metadata": {
        "id": "RsSrIP6sNgs0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "# You can create the tool to pass to an agent\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run,\n",
        ")\n",
        "\n",
        "# Prepare tools\n",
        "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
        "tools.append(search_tool)"
      ],
      "metadata": {
        "id": "7M0fe4ptOQfV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ],
      "metadata": {
        "id": "HFkxgeTYOSJN"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the Price of a MacBook Pro?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a iPhone 16 Pro Max in CAD? How much would it cost in INR if the exchange rate is 60.26 INR for 1 CAD?\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8XmZT3bOVNR",
        "outputId": "ba9c45a5-b49f-470b-cdba-15ead63c5b2d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find the current price of an iPhone 16 Pro Max in CAD and then convert it to INR using the exchange rate.\n",
            "Action: duckduck\n",
            "Action Input: \"current price of iPhone 16 Pro Max in CAD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: iPhone 16 Plus - $1,279; iPhone 16 Pro - $1,449; iPhone 15 Pro Max - $1,749; Anyway, here's the carrier pricing in no particular order: Rogers. iPhone 16 Pro Max: $0 down, $54/mo financing with Save & Return - outright $1,799. iPhone 16 Pro: $0 down, $44/mo financing with Save & Return - outright $1,490., title: Here's how much the iPhone 16 series costs at Canadian carriers, link: https://mobilesyrup.com/2024/09/13/canadian-carrier-iphone-16-pricing-bell-telus-rogers-freedom/, snippet: We now have iPhone 16, iPhone 16 Plus, iPhone 16 Pro and iPhone 16 Pro Max Canadian pricing. These smartphones are getting even more expensive every year. Check out the full iPhone 16 series Canadian pricing below… iPhone 16. 128GB: $1129; 256GB: $1279; 512GB: $1579; iPhone 16 Plus. 128GB: $1279; 256GB: $1429; 512GB: $1729, title: Here's iPhone 16, iPhone 16 Pro Canadian Pricing, link: https://www.iphoneincanada.ca/2024/09/09/iphone-16-pro-canadian-pricing/, snippet: Those that made first pre-orders for the iPhone 16, iPhone 16 Plus, iPhone 16 Pro and iPhone 16 Pro Max should be receiving them today, while in-store pick up is also available. Canadian iPhone 16 series pricing is as follows: iPhone 16. 128GB: $1129; 256GB: $1279; 512GB: $1579; iPhone 16 Plus. 128GB: $1279; 256GB: $1429; 512GB: $1729; iPhone ..., title: You Can Now Buy iPhone 16, iPhone 16 Pro in Canada, link: https://www.iphoneincanada.ca/2024/09/20/you-can-now-buy-iphone-16-iphone-16-pro-in-canada/, snippet: These devices are now available for pre-order except for the iPhone 16 handsets. The smartphones become available for pre-order on Friday. All devices launch on September 20th. Update Sept. 13, 2024 at 11:40am ET: You can find carrier pricing for the iPhone 16 series here. Check out the pricing below: iPhone 16 Pro, title: Here's the Canadian pricing for the iPhone 16, 16 Pro and more, link: https://mobilesyrup.com/2024/09/09/canadian-pricing-iphone-16-16pro/\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of an iPhone 16 Pro Max in CAD, now I need to convert it to INR using the exchange rate.\n",
            "Action: Calculator\n",
            "Action Input: 1799 * 60.26\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 108407.73999999999\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The current price of an iPhone 16 Pro Max in CAD is $1799, which would cost approximately 108,407.74 INR at an exchange rate of 60.26 INR for 1 CAD.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a iPhone 16 Pro Max in CAD? How much would it cost in INR if the exchange rate is 60.26 INR for 1 CAD?',\n",
              " 'output': 'The current price of an iPhone 16 Pro Max in CAD is $1799, which would cost approximately 108,407.74 INR at an exchange rate of 60.26 INR for 1 CAD.'}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qaN7-NPOWrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}